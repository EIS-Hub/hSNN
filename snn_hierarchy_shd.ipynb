{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SNN with hierarchy of Time constant\n",
    "---\n",
    "This notebook is mainly for developing the code that will be rewritten in python files\\\\\n",
    "\n",
    "The idea is to explore what role a hierarchy of time scales have in temporal processing, especially when dealing with multi-time-scale inputs.\\\\\n",
    "\n",
    "Prior literature shows the importance of heterogeneity of time scales in SNNs, mainly showing that diversity of time-scales are beneficial.\n",
    "- https://www.nature.com/articles/s41467-021-26022-3\n",
    "- https://www.nature.com/articles/s41467-023-44614-z\n",
    "\n",
    "However, an interpretation of the role of the hierarchy of time constant is still missing.\n",
    "\n",
    "The assumption is that when treating a temporal sequence sempled at a certain period $\\tau_{sampling}$ and with a total duration $\\Tau$, there can be an optimal sequence of filters $F_i(\\tau_i)$ that process the input sequence. For each of the filters $\\tau_{i+1} > \\tau_i$, where the subscript indicates the order of the filter. More simply, there is a hierarchy of time-scales - from fast to slow - that leads to an optimal processing of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1708624140.694250       1 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[CpuDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.example_libraries import optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import lognorm, norm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import random\n",
    "\n",
    "import urllib.request\n",
    "import gzip, shutil\n",
    "import hashlib\n",
    "import h5py\n",
    "from six.moves.urllib.error import HTTPError\n",
    "from six.moves.urllib.error import URLError\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "from jax import vmap, pmap, jit, value_and_grad, local_device_count\n",
    "from jax.example_libraries import optimizers\n",
    "from jax.lax import scan, cond\n",
    "import pickle\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".75\" # needed because network is huge\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "np.set_printoptions(threshold=100000000)\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters of the SNN model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimArgs:\n",
    "    def __init__(self):\n",
    "        # archi\n",
    "        self.n_in = 700\n",
    "        self.n_out = 20\n",
    "        self.n_layers = 3\n",
    "        self.n_hid = 128\n",
    "        # weight\n",
    "        self.w_scale = 0.3\n",
    "        self.pos_w = False # use only positive weights at initizialization\n",
    "        self.noise_sd = 0 # [0.05, 0.1, 0.15, 0.2]\n",
    "        # data\n",
    "        self.nb_rep = 1\n",
    "        self.nb_steps = 250 #int( np.round( self.time_max/self.timestep, 0 ) )\n",
    "        self.time_max = 1.4 # second\n",
    "        self.timestep = self.time_max/self.nb_steps # 0.014 #0.005 # second, 280 timesteps\n",
    "        self.pert_proba = None\n",
    "        self.truncation = False # to use only 150 of 280 timesteps \n",
    "        # neuron model\n",
    "        self.tau_start = 4*self.timestep # second\n",
    "        self.tau_end   = self.time_max/4 # second\n",
    "        self.tau_mem = 40e-3\n",
    "        self.distrib_tau = True\n",
    "        self.hierarchy_tau = True\n",
    "        self.train_alpha = True\n",
    "        self.v_rest = 0 \n",
    "        self.v_thr = 1\n",
    "        self.v_reset = 0\n",
    "        # training\n",
    "        self.lr = 0.01\n",
    "        self.nb_epochs = 5\n",
    "        self.grad_clip = 1000\n",
    "        self.batch_size = 128\n",
    "        self.seed = 42\n",
    "        self.lr_config = 2 \n",
    "\n",
    "args = SimArgs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Import the SHD dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_dataset(cache_dir, cache_subdir, dataset_name):\n",
    "    # The remote directory with the data files\n",
    "    base_url = \"https://zenkelab.org/datasets\"\n",
    "\n",
    "    # Retrieve MD5 hashes from remote\n",
    "    response = urllib.request.urlopen(f\"{base_url}/md5sums.txt\")\n",
    "    data = response.read()\n",
    "    lines = data.decode('utf-8').split(\"\\n\")\n",
    "    file_hashes = {line.split()[1]: line.split()[0] for line in lines if len(line.split()) == 2}\n",
    "\n",
    "    # Download the Spiking Heidelberg Digits (SHD) dataset\n",
    "    if dataset_name == 'shd':\n",
    "        files = [ \"shd_train.h5.gz\", \"shd_test.h5.gz\"]\n",
    "    if dataset_name == 'ssc':\n",
    "        files = [ \"ssc_train.h5.gz\", \"ssc_test.h5.gz\"]\n",
    "    if dataset_name == 'all':\n",
    "        files = [ \"shd_train.h5.gz\", \"shd_test.h5.gz\", \"ssc_train.h5.gz\", \"ssc_test.h5.gz\"]\n",
    "\n",
    "    for fn in files:\n",
    "        origin = f\"{base_url}/{fn}\"\n",
    "        hdf5_file_path = get_and_gunzip(origin, fn, md5hash=file_hashes[fn], cache_dir=cache_dir, cache_subdir=cache_subdir)\n",
    "        # print(f\"File {fn} decompressed to:\")\n",
    "        print(f\"Available at: {hdf5_file_path}\")\n",
    "\n",
    "def get_and_gunzip(origin, filename, md5hash=None, cache_dir=None,\n",
    "                   cache_subdir=None):\n",
    "    gz_file_path = get_file(filename, origin, md5_hash=md5hash,\n",
    "                            cache_dir=cache_dir, cache_subdir=cache_subdir)\n",
    "    hdf5_file_path = gz_file_path[:-3]\n",
    "    if not os.path.isfile(hdf5_file_path) or \\\n",
    "            os.path.getctime(gz_file_path) > os.path.getctime(hdf5_file_path):\n",
    "        print(f\"Decompressing {gz_file_path}\")\n",
    "        with gzip.open(gz_file_path, 'r') as f_in, \\\n",
    "                open(hdf5_file_path, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    return hdf5_file_path\n",
    "\n",
    "def get_numpy_datasets(subkey_perturbation, pert_proba, dataset_name, n_inp, cache_dir, download=False, nb_steps=args.nb_steps, truncation=False):\n",
    "    cache_subdir = f\"audiospikes\" #f\"audiospikes_{n_inp}\"\n",
    "    if download:\n",
    "        get_audio_dataset(cache_dir, cache_subdir, dataset_name)\n",
    "\n",
    "    train_ds = []; test_ds = []\n",
    "    if dataset_name in ['shd', 'all']:\n",
    "        train_shd_file = h5py.File(os.path.join(cache_dir, cache_subdir,\n",
    "                                                'shd_train.h5'\n",
    "                                                ), 'r')\n",
    "        test_shd_file  = h5py.File(os.path.join(cache_dir, cache_subdir,\n",
    "                                                'shd_test.h5'\n",
    "                                                ), 'r')\n",
    "        shd_train_ds = DatasetNumpy(train_shd_file['spikes'],\n",
    "                                    train_shd_file['labels'],\n",
    "                                    name='shd', target_dim=n_inp, nb_rep=1, ################################################ nb_rep\n",
    "                                    nb_steps=nb_steps, pert_proba=pert_proba, \n",
    "                                    subkey_perturbation=subkey_perturbation, \n",
    "                                    truncation=truncation) \n",
    "        shd_test_ds  = DatasetNumpy(test_shd_file['spikes'],\n",
    "                                    test_shd_file['labels'],\n",
    "                                    name='shd', target_dim=n_inp, nb_rep=1, ################################################ nb_rep\n",
    "                                    nb_steps=nb_steps, truncation=truncation) \n",
    "        train_ds.append(shd_train_ds)\n",
    "        test_ds.append(shd_test_ds)\n",
    "\n",
    "    # if dataset_name in ['ssc', 'all']:\n",
    "    #     train_ssc_file = h5py.File(os.path.join(cache_dir, cache_subdir,\n",
    "    #                                             'ssc_train.h5'\n",
    "    #                                             ), 'r')\n",
    "    #     test_ssc_file  = h5py.File(os.path.join(cache_dir, cache_subdir,\n",
    "    #                                             'ssc_test.h5'\n",
    "    #                                             ), 'r')\n",
    "    #     ssc_train_ds = DatasetNumpy(train_ssc_file['spikes'],\n",
    "    #                                 train_ssc_file['labels'],\n",
    "    #                                 name='ssc', target_dim=n_inp)\n",
    "    #     ssc_test_ds  = DatasetNumpy(test_ssc_file['spikes'],\n",
    "    #                                 test_ssc_file['labels'],\n",
    "    #                                 name='ssc', target_dim=n_inp)\n",
    "    #     train_ds.append(ssc_train_ds)\n",
    "    #     test_ds.append(ssc_test_ds)\n",
    "\n",
    "    return train_ds, test_ds\n",
    "\n",
    "class DatasetNumpy(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Numpy based generator\n",
    "    \"\"\"\n",
    "    def __init__(self, spikes, labels, name, target_dim, nb_rep, nb_steps, pert_proba=None, subkey_perturbation=None, truncation=False):\n",
    "        print(pert_proba, subkey_perturbation)\n",
    "        self.nb_steps = nb_steps #int(1.4/timestep)   # number of time steps in the input ################################################ nb_steps\n",
    "        # print(f'nb_step: {self.nb_steps} (DatasetNumpyModified.__init__)')\n",
    "        self.nb_units = 700   # number of input units (channels)\n",
    "        self.max_time = 1.4   # maximum recording time of a digit (in s)\n",
    "        self.spikes = spikes  # recover the 'spikes' dictionary from the h5 file\n",
    "        self.labels = labels  # recover the 'labels' array from the h5 file\n",
    "        self.name = name      # name of the dataset or name of speaker\n",
    "\n",
    "        self.firing_times = self.spikes['times']\n",
    "        self.units_fired  = self.spikes['units']\n",
    "        self.num_samples = self.firing_times.shape[0]\n",
    "        self.time_bins = np.linspace(0, self.max_time, num=self.nb_steps)\n",
    "\n",
    "        # initialize the input (3D) and output (1D) arrays\n",
    "        self.input  = np.zeros((self.num_samples, self.nb_steps,\n",
    "                                 self.nb_units), dtype=np.uint8)\n",
    "        self.output = np.array(self.labels, dtype=np.uint8)\n",
    "\n",
    "        self.load_spikes()\n",
    "        self.reduce_inp_dimensions(target_dim=target_dim, axis=2, nb_rep=nb_rep)\n",
    "\n",
    "        if truncation: \n",
    "            self.input = self.input[:, :150,:]\n",
    "            print(f'TRUNCATION: ON')\n",
    "            print(f'nb_step after truncation: {self.input.shape[1]} (DatasetNumpyModified.__init__)')\n",
    "        else: \n",
    "            print(f'TRUNCATION: OFF')\n",
    "\n",
    "        if pert_proba != None:\n",
    "          perturbation = jax.random.bernoulli(subkey_perturbation, p=pert_proba, shape=self.input.shape)\n",
    "          perturbation = jnp.logical_or(self.input, perturbation).astype(jnp.uint8)\n",
    "          self.input = jnp.concatenate([self.input, perturbation], axis=0, dtype=jnp.uint8)\n",
    "          self.output = jnp.tile(self.output, reps=2)\n",
    "          print(f'self.input after perturbation: {self.input.shape} (DatasetNumpyModified.__init__)')\n",
    "        self.num_samples = self.input.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def load_spikes(self):\n",
    "        \"\"\"\n",
    "        For each sample, we create a 2D array of size (nb_steps, nb_units).\n",
    "        We downsample the firing times and the units fired to the time bins\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for idx in range(self.num_samples):\n",
    "            times = np.digitize(self.firing_times[idx], self.time_bins)\n",
    "            units = self.units_fired[idx]\n",
    "            self.input[idx, times, units] = 1\n",
    "\n",
    "    def reduce_inp_dimensions(self, target_dim, axis, nb_rep):\n",
    "        sample_ind = int(np.ceil(self.nb_units / target_dim))\n",
    "        assert nb_rep <= sample_ind, f'The maximum factor of data augmentation is {sample_ind}, you provided {nb_rep}'\n",
    "        index = [np.arange(i, 700, sample_ind) for i in range(sample_ind)]\n",
    "        reshaped = [np.take(self.input, index[i], axis)\n",
    "                    for i in range(nb_rep)] # this samples the data a\n",
    "        reshaped = [np.pad(reshaped[i],\n",
    "                            [(0, 0), (0, 0),\n",
    "                             (0, int(target_dim-reshaped[i].shape[2]))],\n",
    "                            mode='constant')\n",
    "                    for i in range(nb_rep)]\n",
    "        reshaped = np.concatenate(reshaped, axis=0)\n",
    "\n",
    "        self.input = reshaped\n",
    "        self.output = np.tile(self.output, nb_rep)\n",
    "        self.num_samples = reshaped.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs, outputs = self.__data_generation(idx)\n",
    "        return inputs, outputs\n",
    "\n",
    "    def __data_generation(self, idx):\n",
    "        if self.name == 'shd':\n",
    "            output = self.output[idx]\n",
    "        if self.name == 'ssc':\n",
    "            output = self.output[idx] + 20\n",
    "        return self.input[idx], output\n",
    "\n",
    "def get_file(fname,\n",
    "             origin,\n",
    "             md5_hash=None,\n",
    "             file_hash=None,\n",
    "             cache_subdir='datasets',\n",
    "             hash_algorithm='auto',\n",
    "             extract=False,\n",
    "             archive_format='auto',\n",
    "             cache_dir=None):\n",
    "    if cache_dir is None:\n",
    "        cache_dir = os.path.join(os.path.expanduser('~'), '.data-cache')\n",
    "    if md5_hash is not None and file_hash is None:\n",
    "        file_hash = md5_hash\n",
    "        hash_algorithm = 'md5'\n",
    "    datadir_base = os.path.expanduser(cache_dir)\n",
    "    if not os.access(datadir_base, os.W_OK):\n",
    "        datadir_base = os.path.join('/tmp', '.data-cache')\n",
    "    datadir = os.path.join(datadir_base, cache_subdir)\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    os.makedirs(datadir, exist_ok=True)\n",
    "\n",
    "    fpath = os.path.join(datadir, fname)\n",
    "\n",
    "    download = False\n",
    "    if os.path.exists(fpath):\n",
    "    # File found; verify integrity if a hash was provided.\n",
    "        if file_hash is not None:\n",
    "            if not validate_file(fpath, file_hash, algorithm=hash_algorithm):\n",
    "                print('A local file was found, but it seems to be '\n",
    "                      'incomplete or outdated because the ' + hash_algorithm +\n",
    "                      ' file hash does not match the original value of ' + file_hash +\n",
    "                      ' so we will re-download the data.')\n",
    "                download = True\n",
    "    else:\n",
    "        download = True\n",
    "\n",
    "    if download:\n",
    "        print('Downloading data from', origin)\n",
    "\n",
    "        error_msg = 'URL fetch failure on {}: {} -- {}'\n",
    "        try:\n",
    "            try:\n",
    "                urlretrieve(origin, fpath)\n",
    "            except HTTPError as e:\n",
    "                raise Exception(error_msg.format(origin, e.code, e.msg))\n",
    "            except URLError as e:\n",
    "                raise Exception(error_msg.format(origin, e.errno, e.reason))\n",
    "        except (Exception, KeyboardInterrupt) as e:\n",
    "            if os.path.exists(fpath):\n",
    "                os.remove(fpath)\n",
    "\n",
    "    return fpath\n",
    "\n",
    "def validate_file(fpath, file_hash, algorithm='auto', chunk_size=65535):\n",
    "    if (algorithm == 'sha256') or \\\n",
    "            (algorithm == 'auto' and len(file_hash) == 64):\n",
    "        hasher = 'sha256'\n",
    "    else:\n",
    "        hasher = 'md5'\n",
    "\n",
    "    if str(_hash_file(fpath, hasher, chunk_size)) == str(file_hash):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def _hash_file(fpath, algorithm='sha256', chunk_size=65535):\n",
    "    if (algorithm == 'sha256') or (algorithm == 'auto' and len(hash) == 64):\n",
    "        hasher = hashlib.sha256()\n",
    "    else:\n",
    "        hasher = hashlib.md5()\n",
    "\n",
    "    with open(fpath, 'rb') as fpath_file:\n",
    "        for chunk in iter(lambda: fpath_file.read(chunk_size), b''):\n",
    "            hasher.update(chunk)\n",
    "\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def custom_collate_fn(batch): \n",
    "  transposed_data = list(zip(*batch))\n",
    "\n",
    "  labels = np.array(transposed_data[1])\n",
    "  spikes = np.array(transposed_data[0])\n",
    "\n",
    "  return spikes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None [255383827 267815257]\n",
      "TRUNCATION: OFF\n",
      "None None\n",
      "TRUNCATION: OFF\n",
      "8156\n",
      "8156 6524 1632 8156\n"
     ]
    }
   ],
   "source": [
    "cache_dir = '/Users/filippomoro/Desktop/KINGSTONE/Datasets/SHD' # take data from tristan, to avoid copies #os.getcwd()\n",
    "key = jax.random.PRNGKey(args.seed)\n",
    "key, subkey_perturbation = jax.random.split(key)\n",
    "train_ds, test_ds = get_numpy_datasets(subkey_perturbation, args.pert_proba, 'shd', args.n_in, cache_dir=cache_dir, download=False, nb_steps=args.nb_steps, truncation=args.truncation)\n",
    "print(len(train_ds[0]))\n",
    "\n",
    "train_ds = train_ds[0]\n",
    "test_ds = test_ds[0]\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "train_size = int(0.8 * len(train_ds))\n",
    "val_size   = len(train_ds) - train_size\n",
    "train_ds_split, val_ds_split = random_split(train_ds, [train_size, val_size])\n",
    "print(len(train_ds), len(train_ds_split), len(val_ds_split), len(train_ds_split)+len(val_ds_split))\n",
    "\n",
    "train_loader_custom_collate = DataLoader(train_ds_split, args.batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader_custom_collate   = DataLoader(val_ds_split,   args.batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "test_loader_custom_collate  = DataLoader(test_ds,        args.batch_size, shuffle=None, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Model\n",
    "---\n",
    "We'll use Leaky-Integrate-and-Fire neurons for the Hidden layers, Leaky-Integrator neurons for the output.\n",
    "\n",
    "We also define some additional functions to: Introduce Noise in Weights and Membrane voltage, Introduce the Surrogate Gradient function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Noise function\n",
    "@jax.custom_jvp\n",
    "def add_noise(w, key, noise_std):\n",
    "    ''' Adds noise only for inference '''\n",
    "    noisy_w = jnp.where(w != 0.0,\n",
    "                        w + jax.random.normal(key, w.shape) * jnp.max(jnp.abs(w)) * noise_std,\n",
    "                        w)\n",
    "    return noisy_w\n",
    "\n",
    "@add_noise.defjvp\n",
    "def add_noise_jvp(primals, tangents):\n",
    "    weight, key, noise_std = primals\n",
    "    x_dot, y_dot, z_dot = tangents\n",
    "    primal_out = add_noise(weight, key, noise_std)\n",
    "    tangent_out = x_dot\n",
    "    return primal_out, tangent_out\n",
    "\n",
    "\n",
    "### Surrogate Gradient function\n",
    "@jax.custom_jvp\n",
    "def spiking_fn(x, thr):\n",
    "    \"\"\" Thresholding function for spiking neurons. \"\"\"\n",
    "    return (x > thr).astype(jnp.float32)\n",
    "\n",
    "@spiking_fn.defjvp\n",
    "def spiking_jpv(primals, tangents):\n",
    "    \"\"\" Surrogate gradient function for thresholding. \"\"\"\n",
    "    x, thr = primals\n",
    "    x_dot, y_dot = tangents\n",
    "    primal_out = spiking_fn(x, thr)\n",
    "    tangent_out = x_dot / (10 * jnp.absolute(x - thr) + 1)**2\n",
    "    return primal_out, tangent_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def params_initializer( key, args ):\n",
    "    \"\"\" Initialize parameters. \"\"\"\n",
    "    key_hid = jax.random.split(key, args.n_layers); key=key_hid[0]; key_hid=key_hid[1:]\n",
    "\n",
    "    # Initializing the weights, weight masks and time constant (alpha factors)\n",
    "    net_params, net_states = [], []\n",
    "    for l in range(args.n_layers):\n",
    "        if l == 0:\n",
    "            n_pre = args.n_in; n_post = args.n_hid\n",
    "\n",
    "            # partition of the time constants in the different layers\n",
    "            if args.distrib_tau:\n",
    "                tau_l = jax.random.uniform(key_hid[l], [args.n_hid], minval=0.5*(args.tau_start + (l/args.n_layers)*( args.tau_end-args.tau_start )), maxval=1.5*(args.tau_start + (l/args.n_layers)*( args.tau_end-args.tau_start ))  )\n",
    "            else:\n",
    "                tau_l = args.tau_start + (l/args.n_layers)*( args.tau_end-args.tau_start )\n",
    "            alpha_l = jnp.exp(-args.timestep/tau_l)\n",
    "\n",
    "        elif l == args.n_layers-1:\n",
    "            n_pre = args.n_hid; n_post = args.n_out\n",
    "            # same time-constant for output neurons \n",
    "            tau_l = args.tau_start + (l/args.n_layers)*( args.tau_end-args.tau_start )\n",
    "            alpha_l = jnp.exp(-args.timestep/tau_l)\n",
    "            \n",
    "        else:\n",
    "            n_pre = args.n_hid; n_post = args.n_hid\n",
    "            # partition of the time constants in the different layers\n",
    "            if args.distrib_tau:\n",
    "                tau_l = jax.random.uniform(key_hid[l], [args.n_hid], minval=0.5*(args.tau_start + (l/args.n_layers)*( args.tau_end-args.tau_start )), maxval=1.5*(args.tau_start + (l/args.n_layers)*( args.tau_end-args.tau_start ))  )\n",
    "            else:\n",
    "                tau_l = args.tau_start + (l/args.n_layers)*( args.tau_end-args.tau_start )\n",
    "            alpha_l = jnp.exp(-args.timestep/tau_l)\n",
    "\n",
    "        # flat tau and alpha is the temporal hierarchy is not formed\n",
    "        if not args.hierarchy_tau:\n",
    "            tau_l = args.tau_start + (1/args.n_layers)*( args.tau_end-args.tau_start )\n",
    "            alpha_l = jnp.exp(-args.timestep/tau_l)\n",
    "\n",
    "        # initializing the hidden weights with a normal distribution\n",
    "        weight_l = jax.random.normal(key_hid[l], [n_pre, n_post]) * args.w_scale\n",
    "        weight_mask_l = 1 # jax.random.uniform(key_hid[l], [n_pre, n_post]) < (1/args.n_layers)\n",
    "\n",
    "        # the initialization of the membrane voltage\n",
    "        v_mems = np.zeros( (n_post) )\n",
    "        out_spikes = np.zeros( (n_post) )\n",
    "\n",
    "        # building the parameters for each layer\n",
    "        net_params.append( [weight_l, alpha_l] )\n",
    "        net_states.append( [weight_mask_l, tau_l, v_mems, out_spikes, args.v_thr, args.noise_sd] )\n",
    "\n",
    "    return net_params, net_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lif_forward(net_params, net_states, input_spikes):\n",
    "    ''' Forward function for the Leaky-Integrate and Fire neuron layer, adopted here for the hidden layers. '''\n",
    "\n",
    "    # state: the parameters (weights) and the state of the neurons (spikes, inputs and membrane, ecc..)\n",
    "    # if train_alpha: w, alpha = net_params; w_mask, tau, V_mem, out_spikes, v_thr, noise_sd = net_states\n",
    "    # else: w = net_params; alpha, w_mask, tau, V_mem, out_spikes, v_thr, noise_sd = net_states\n",
    "    w, alpha = net_params; w_mask, tau, V_mem, out_spikes, v_thr, noise_sd = net_states\n",
    "\n",
    "    # we evolve the state of the neuron according to the LIF formula, Euler approximation\n",
    "    I_in = jnp.matmul(input_spikes, w*w_mask) #jnp.einsum('ij,ij->i', w*w_mask, input_spikes)\n",
    "    V_mem = (1-alpha) * (V_mem) + (alpha) * I_in - out_spikes*v_thr\n",
    "    out_spikes = spiking_fn( V_mem, v_thr )\n",
    "    \n",
    "    #V_mem = jnp.maximum(0, V_mem) # HW constraint\n",
    "    # if train_alpha:\n",
    "    #     return [w, alpha], [w_mask, tau, V_mem, out_spikes, v_thr, noise_sd]\n",
    "    # else:\n",
    "    #     return w, [alpha, w_mask, tau, V_mem, out_spikes, v_thr, noise_sd]\n",
    "    return [w, alpha], [w_mask, tau, V_mem, out_spikes, v_thr, noise_sd]\n",
    "\n",
    "def li_output(net_params, net_states, input_spikes):\n",
    "    ''' Forward function for the Leaky-Integrator neuron layer, adopted here for the output layers. '''\n",
    "\n",
    "    # state: the parameters (weights) and the state of the neurons (inputs and membrane)\n",
    "    # if train_alpha: w, alpha = net_params; w_mask, tau, V_mem, out_spikes, v_thr, noise_sd = net_states\n",
    "    # else: w = net_params; alpha, w_mask, tau, V_mem, out_spikes, v_thr, noise_sd = net_states\n",
    "    w, alpha = net_params; w_mask, tau, V_mem, out_spikes, v_thr, noise_sd = net_states\n",
    "\n",
    "    # we evolve the state of the neuron according to the LI formula, Euler approximation\n",
    "    I_in = jnp.matmul(input_spikes, w*w_mask)\n",
    "    V_mem = (1-alpha) * (V_mem) + (alpha) * I_in\n",
    "    \n",
    "    #V_mem = jnp.maximum(0, V_mem) # HW constraint\n",
    "    # if train_alpha:\n",
    "    #     return [w, alpha], [w_mask, tau, V_mem, out_spikes, v_thr, noise_sd]\n",
    "    # else:\n",
    "    #     return w, [alpha, w_mask, tau, V_mem, out_spikes, v_thr, noise_sd]\n",
    "    return [w, alpha], [w_mask, tau, V_mem, out_spikes, v_thr, noise_sd]\n",
    "\n",
    "def hsnn_step( args_in, input_spikes):\n",
    "    '''The Hierarchical time-constant SNN (hSNN). Made of n_layers layers.'''\n",
    "    net_params, net_states = args_in\n",
    "    n_layers = len(net_params)\n",
    "    # First layer takes inputs from \"input spikes\"\n",
    "    net_params[0], net_states[0] = lif_forward( net_params[0], net_states[0], input_spikes )\n",
    "    for l in range(1, n_layers-1):\n",
    "        # Hidden layer takes inputs from their previous layer\n",
    "        net_params[l], net_states[l] = lif_forward( net_params[l], net_states[l], net_states[l-1][3] ) # net_params[l-1][5] : output spikes from previous layer\n",
    "    # Output layer is a leaky integrator (LI)\n",
    "    net_params[-1], net_states[-1] = li_output( net_params[-1], net_states[-1], net_states[l-1][3] )\n",
    "\n",
    "    return [net_params, net_states], net_states # net_params[-1][4] : output leaky membrane voltage\n",
    "\n",
    "def decoder_sum( out_v_mem ):\n",
    "    return jax.nn.softmax( jnp.mean( out_v_mem, axis=1 ), axis=-1 )\n",
    "\n",
    "def decoder_cum( out_v_mem ):\n",
    "    return jnp.mean( jax.nn.softmax( out_v_mem, axis=-1 ), axis=1)\n",
    "\n",
    "def decoder_vmax( out_v_mem ):\n",
    "    return jax.nn.softmax( jnp.max( out_v_mem, axis=1 ), axis=-1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: train (128, 100, 700) - test (128, 100, 700)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### try and do a forward pass\n",
    "# load data\n",
    "x_train, Y = next(iter( train_loader_custom_collate ))\n",
    "x_test, Y_test = next(iter( train_loader_custom_collate ))\n",
    "print('Input shape: train '+ str(x_train.shape) + ' - test '+ str(x_test.shape) )\n",
    "\n",
    "# initialize parameters\n",
    "net_params, net_states = params_initializer( key, args )\n",
    "\n",
    "# forward pass: LIF layer\n",
    "net_params[0], net_states[0] = lif_forward( net_params[0], net_states[0], x_train[0,50] )\n",
    "\n",
    "# forward pass: network\n",
    "args_out = hsnn_step( [net_params, net_states], x_train[0,50] )\n",
    "len(args_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 20) (128, 20)\n"
     ]
    }
   ],
   "source": [
    "@jit\n",
    "def predict(args_in, X):\n",
    "    \"\"\" Scans over time and return predictions. \"\"\"\n",
    "    # net_params, net_states = args_in\n",
    "    _, net_states_hist = scan(hsnn_step, args_in, X, length=args.nb_steps)\n",
    "    return net_states_hist\n",
    "# vmap the forward of the model\n",
    "v_predict = vmap(predict, in_axes=(None, 0))\n",
    "\n",
    "def one_hot(x, n_class):\n",
    "    return jnp.array(x[:, None] == jnp.arange(n_class), dtype=jnp.float32)\n",
    "\n",
    "def loss(key, net_params, net_states, X, Y, epoch):\n",
    "    \"\"\" Calculates CE loss after predictions. \"\"\"\n",
    "\n",
    "    # we might want to add noise in the forward pass --> memristor-aware-training\n",
    "    # weight = [net_params[i][0] for i in range( len(net_params) )]\n",
    "    # weight = cond(\n",
    "    #     epoch >= noise_start_step, \n",
    "    #     lambda weight, key : add_noise(weight, key, noise_std),\n",
    "    #     lambda weight, key : weight,\n",
    "    #     weight, key\n",
    "    # )\n",
    "    # forward pass\n",
    "    net_states_hist = v_predict( [net_params, net_states], X)\n",
    "    out_v_mem = net_states_hist[-1][2]\n",
    "    Yhat = decoder( out_v_mem )\n",
    "    # compute the loss and correct examples\n",
    "    num_correct = jnp.sum(jnp.equal(jnp.argmax(Yhat, 1), jnp.argmax(Y, 1)))\n",
    "    loss_ce = -jnp.mean(jnp.sum( jnp.log(Yhat * Y + 1e-8), axis=-1, dtype=jnp.float32))\n",
    "    # loss_fr = np.mean(target_fr - 10 * np.mean(out_v_mem)) ** 2\n",
    "    ################# ----> Do I need the spiking frequency regularizer?\n",
    "    loss_total = loss_ce #+ loss_fr * lambda_fr\n",
    "    loss_values = [num_correct, 10 * np.mean(out_v_mem), loss_ce]\n",
    "    return loss_total, loss_values\n",
    "\n",
    "# testing the training function\n",
    "args_ins = [net_params, net_states]\n",
    "args_out = scan(hsnn_step, args_ins, x_train[0], length=args.nb_steps)\n",
    "[net_params_hist, net_states_hist] = args_out\n",
    "\n",
    "net_states_hist = v_predict( [net_params, net_states], x_train )\n",
    "out_v_mem = net_states_hist[-1][2]\n",
    "\n",
    "# decoder\n",
    "Yhat = jax.nn.softmax( jnp.mean( out_v_mem, axis=1 ), axis=-1 )\n",
    "Yhat_vmax = jax.nn.softmax( jnp.max( out_v_mem, axis=1 ), axis=-1 )\n",
    "Yhat_cum = jnp.mean( jax.nn.softmax( out_v_mem, axis=-1 ), axis=1)\n",
    "print( Yhat_cum.shape )\n",
    "\n",
    "# loss\n",
    "loss_total, loss_values = loss(key, net_params, net_states, x_train, one_hot(Y, 20), 0)\n",
    "\n",
    "# values and gradients\n",
    "value, grads = value_and_grad(loss, has_aux=True, argnums=(1))(key, net_params, net_states, x_train, one_hot(Y,20), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 100, 20)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_v_mem.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mosaic(key, n_batch, n_epochs, args, \n",
    "                 lr, lr_dropstep, train_dl, test_dl, val_dl,\n",
    "                 model, param_initializer, decoder, \n",
    "                 noise_start_step, noise_std,\n",
    "                 target_fr, lambda_fr, dataset_name):\n",
    "    \n",
    "    key, key_model = jax.random.split(key, 2)\n",
    "\n",
    "    @jit\n",
    "    def predict(args_in, X):\n",
    "        \"\"\" Scans over time and return predictions. \"\"\"\n",
    "        _, net_states_hist = scan(model, args_in, X, length=args.nb_steps)\n",
    "        return net_states_hist\n",
    "    # vmap the forward of the model\n",
    "    v_predict = vmap(predict, in_axes=(None, 0))\n",
    "\n",
    "    def loss(key, net_params, net_states, X, Y, epoch):\n",
    "        \"\"\" Calculates CE loss after predictions. \"\"\"\n",
    "\n",
    "        # we might want to add noise in the forward pass --> memristor-aware-training\n",
    "        # weight = [net_params[i][0] for i in range( len(net_params) )]\n",
    "        # weight = cond(\n",
    "        #     epoch >= noise_start_step, \n",
    "        #     lambda weight, key : add_noise(weight, key, noise_std),\n",
    "        #     lambda weight, key : weight,\n",
    "        #     weight, key\n",
    "        # )\n",
    "        # forward pass\n",
    "        net_states_hist = v_predict( [net_params, net_states], X)\n",
    "        out_v_mem = net_states_hist[-1][2]\n",
    "        Yhat = decoder( out_v_mem )\n",
    "        # compute the loss and correct examples\n",
    "        num_correct = jnp.sum(jnp.equal(jnp.argmax(Yhat, 1), jnp.argmax(Y, 1)))\n",
    "        loss_ce = -jnp.mean( jnp.log( jnp.sum( Yhat * Y, axis=-1, dtype=jnp.float32) + 1e-8 ) )\n",
    "        # loss_fr = np.mean(target_fr - 10 * np.mean(out_v_mem)) ** 2\n",
    "        ################# ----> Do I need the spiking frequency regularizer?\n",
    "        loss_total = loss_ce #+ loss_fr * lambda_fr\n",
    "        loss_values = [num_correct, loss_ce]\n",
    "        return loss_total, loss_values\n",
    " \n",
    "    @jit\n",
    "    def update(key, epoch, net_states, X, Y, opt_state):\n",
    "        train_params = get_params(opt_state)\n",
    "        # forward pass with gradients\n",
    "        value, grads = value_and_grad(loss, has_aux=True, argnums=(1))(key, train_params, net_states, X, Y, epoch)\n",
    "        # possibly disable gradients on alpha and gradient clip\n",
    "        for g in range( len( grads ) ):\n",
    "            grads[g][0] = np.clip(grads[g][0], -args.grad_clip, args.grad_clip)\n",
    "            grads[g][1] = np.clip(grads[g][1], -args.grad_clip, args.grad_clip)\n",
    "        return grads, opt_state, value\n",
    "\n",
    "    def one_hot(x, n_class):\n",
    "        return np.array(x[:, None] == np.arange(n_class), dtype=np.float32)\n",
    "\n",
    "    def total_correct(net_params, net_states, X, Y):\n",
    "        net_states_hist = v_predict( [net_params, net_states], X)\n",
    "        out_v_mem = net_states_hist[-1][2]\n",
    "        Yhat = decoder( out_v_mem )\n",
    "        acc = np.sum(np.equal(np.argmax(Yhat, 1), Y))\n",
    "        return acc\n",
    "\n",
    "    pw_lr = optimizers.piecewise_constant([lr_dropstep], [lr, lr/10])\n",
    "    # define the optimizer\n",
    "    opt_init, opt_update, get_params = optimizers.adam(step_size=pw_lr)\n",
    "    # initialize the parameters (and states)\n",
    "    net_params, net_states = param_initializer( key_model, args )\n",
    "    opt_state = opt_init(net_params)\n",
    "\n",
    "    # Training loop\n",
    "    train_loss = []\n",
    "    train_step = 0\n",
    "    for epoch in range(n_epochs):\n",
    "        t = time.time()\n",
    "        acc = 0; count = 0\n",
    "        for batch_idx, (x, y) in enumerate(train_dl):\n",
    "            y = one_hot(y, args.n_out)\n",
    "            key, _ = jax.random.split(key)\n",
    "            grads, opt_state, (L, [tot_correct, _]) = update(key, epoch, net_states, x, y, opt_state)\n",
    "            # possibly remove gradient from alpha\n",
    "            if not args.train_alpha: \n",
    "                for g in range(len(grads)): grads[g][1] *= 0\n",
    "            # weight update\n",
    "            opt_state = opt_update(0, grads, opt_state)\n",
    "            net_params = get_params(opt_state)\n",
    "            # clip alpha between 0 and 1\n",
    "            if args.train_alpha:\n",
    "                for g in range(len(net_params)): net_params[g][1] = jnp.clip(net_params[g][1], 0, 1)\n",
    "            # append stats\n",
    "            train_loss.append(L)\n",
    "            train_step += 1\n",
    "            acc += tot_correct\n",
    "            count += x.shape[0]\n",
    "        \n",
    "        # Training logs\n",
    "        train_acc = 100*acc/count\n",
    "        elapsed_time = time.time() - t\n",
    "        print(f'Epoch: [{epoch+1}/{n_epochs}] - Loss: {L:.2f} - '\n",
    "              f'Training acc: {train_acc:.2f} - t: {elapsed_time:.2f} sec')\n",
    "        # if epoch % 50 == 0:\n",
    "        #     # Save training state\n",
    "        #     trained_params = optimizers.unpack_optimizer_state(opt_state)\n",
    "        #     checkpoint_path = os.path.join('checkpoints', \"checkpoint.pkl\")\n",
    "        #     with open(checkpoint_path, \"wb\") as file:\n",
    "        #         pickle.dump(trained_params, file)\n",
    "\n",
    "    # Testing Loop\n",
    "    if dataset_name == 'shd':\n",
    "        shd_test_loader = test_dl\n",
    "        shd_val_loader = val_dl\n",
    "    elif dataset_name == 'ssc':\n",
    "        ssc_test_loader = test_dl\n",
    "        ssc_val_loader = val_dl\n",
    "    elif dataset_name == 'all':\n",
    "        shd_test_loader, ssc_test_loader = test_dl\n",
    "\n",
    "    # SHD\n",
    "    acc = 0; val_acc_shd = 0; count = 0\n",
    "    if dataset_name in ['shd', 'all']:\n",
    "        for batch_idx, (x, y) in enumerate(shd_val_loader):\n",
    "            count += x.shape[0]\n",
    "            acc += total_correct(net_params, net_states, x, y)\n",
    "        val_acc_shd = 100*acc/count\n",
    "        print(f'SHD Validation Accuracy: {val_acc_shd:.2f}')\n",
    "\n",
    "    acc = 0; test_acc_shd = 0; count = 0\n",
    "    if dataset_name in ['shd', 'all']:\n",
    "        for batch_idx, (x, y) in enumerate(shd_test_loader):\n",
    "            count += x.shape[0]\n",
    "            acc += total_correct(net_params, net_states, x, y)\n",
    "        test_acc_shd = 100*acc/count\n",
    "        print(f'SHD Test Accuracy: {test_acc_shd:.2f}')\n",
    "\n",
    "    # SSC\n",
    "    acc = 0 ; test_acc_ssc = 0\n",
    "    if dataset_name in ['ssc', 'all']:\n",
    "        for batch_idx, (x, y) in enumerate(ssc_test_loader):\n",
    "            acc += total_correct(net_params, net_states, x, y)\n",
    "        test_acc_ssc = 100*acc/((batch_idx+1)*n_batch)\n",
    "        print(f'SSC Test Accuracy: {test_acc_ssc:.2f}')\n",
    "\n",
    "    return train_loss, test_acc_shd, test_acc_ssc, val_acc_shd, net_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.lr = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/20] - Loss: 2.36 - Training acc: 20.59 - t: 12.44 sec\n",
      "Epoch: [2/20] - Loss: 1.89 - Training acc: 41.16 - t: 10.29 sec\n",
      "Epoch: [3/20] - Loss: 1.62 - Training acc: 53.11 - t: 9.73 sec\n",
      "Epoch: [4/20] - Loss: 1.45 - Training acc: 59.24 - t: 10.11 sec\n",
      "Epoch: [5/20] - Loss: 1.46 - Training acc: 63.99 - t: 9.94 sec\n",
      "Epoch: [6/20] - Loss: 1.20 - Training acc: 68.07 - t: 10.19 sec\n",
      "Epoch: [7/20] - Loss: 1.26 - Training acc: 69.88 - t: 9.98 sec\n",
      "Epoch: [8/20] - Loss: 1.14 - Training acc: 72.24 - t: 9.99 sec\n",
      "Epoch: [9/20] - Loss: 1.13 - Training acc: 73.27 - t: 10.31 sec\n",
      "Epoch: [10/20] - Loss: 0.98 - Training acc: 75.87 - t: 10.22 sec\n",
      "Epoch: [11/20] - Loss: 1.05 - Training acc: 76.66 - t: 10.01 sec\n",
      "Epoch: [12/20] - Loss: 0.97 - Training acc: 77.53 - t: 9.98 sec\n",
      "Epoch: [13/20] - Loss: 0.93 - Training acc: 78.54 - t: 9.98 sec\n",
      "Epoch: [14/20] - Loss: 1.00 - Training acc: 79.18 - t: 10.16 sec\n",
      "Epoch: [15/20] - Loss: 0.79 - Training acc: 80.23 - t: 9.96 sec\n",
      "Epoch: [16/20] - Loss: 0.95 - Training acc: 80.86 - t: 9.98 sec\n",
      "Epoch: [17/20] - Loss: 0.85 - Training acc: 81.77 - t: 9.91 sec\n",
      "Epoch: [18/20] - Loss: 0.79 - Training acc: 82.19 - t: 9.93 sec\n",
      "Epoch: [19/20] - Loss: 0.78 - Training acc: 83.12 - t: 9.97 sec\n",
      "Epoch: [20/20] - Loss: 0.72 - Training acc: 84.01 - t: 9.96 sec\n",
      "SHD Validation Accuracy: 73.28\n",
      "SHD Test Accuracy: 57.46\n"
     ]
    }
   ],
   "source": [
    "args.train_alpha = True\n",
    "args.hierarchy_tau = True\n",
    "args.distrib_tau = True\n",
    "args.tau_mem = 0.04\n",
    "train_loss, test_acc_shd, test_acc_ssc, val_acc_shd, net_params_trained = train_mosaic(key = jax.random.PRNGKey(args.seed), n_batch=args.batch_size, n_epochs=20, args = args, \n",
    "                                                                lr = args.lr, lr_dropstep=1., \n",
    "                                                                train_dl = train_loader_custom_collate, test_dl = test_loader_custom_collate, val_dl=val_loader_custom_collate,\n",
    "                                                                model=hsnn_step, param_initializer=params_initializer, decoder=decoder_sum, \n",
    "                                                                noise_start_step=10, noise_std=0.1,\n",
    "                                                                target_fr=None, lambda_fr=None, dataset_name='shd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(0.9131007, dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_params_trained[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/50] - Loss: 1.76 - Training acc: 19.85 - t: 10.26 sec\n",
      "Epoch: [1/50] - Loss: 1.10 - Training acc: 56.48 - t: 4.48 sec\n",
      "Epoch: [2/50] - Loss: 0.77 - Training acc: 68.75 - t: 4.39 sec\n",
      "Epoch: [3/50] - Loss: 0.69 - Training acc: 76.07 - t: 4.38 sec\n",
      "Epoch: [4/50] - Loss: 0.72 - Training acc: 79.63 - t: 4.41 sec\n",
      "Epoch: [5/50] - Loss: 0.57 - Training acc: 82.14 - t: 4.40 sec\n",
      "Epoch: [6/50] - Loss: 0.54 - Training acc: 84.93 - t: 4.43 sec\n",
      "Epoch: [7/50] - Loss: 0.34 - Training acc: 86.60 - t: 4.51 sec\n",
      "Epoch: [8/50] - Loss: 0.38 - Training acc: 88.94 - t: 4.52 sec\n",
      "Epoch: [9/50] - Loss: 0.32 - Training acc: 89.51 - t: 4.46 sec\n",
      "Epoch: [10/50] - Loss: 0.34 - Training acc: 90.95 - t: 4.49 sec\n",
      "Epoch: [11/50] - Loss: 0.24 - Training acc: 92.00 - t: 4.43 sec\n",
      "Epoch: [12/50] - Loss: 0.27 - Training acc: 93.08 - t: 4.46 sec\n",
      "Epoch: [13/50] - Loss: 0.19 - Training acc: 94.52 - t: 4.43 sec\n",
      "Epoch: [14/50] - Loss: 0.19 - Training acc: 94.65 - t: 4.46 sec\n",
      "Epoch: [15/50] - Loss: 0.17 - Training acc: 95.47 - t: 4.46 sec\n",
      "Epoch: [16/50] - Loss: 0.20 - Training acc: 96.60 - t: 4.44 sec\n",
      "Epoch: [17/50] - Loss: 0.20 - Training acc: 95.93 - t: 4.44 sec\n",
      "Epoch: [18/50] - Loss: 0.13 - Training acc: 96.89 - t: 4.44 sec\n",
      "Epoch: [19/50] - Loss: 0.13 - Training acc: 97.96 - t: 4.45 sec\n",
      "Epoch: [20/50] - Loss: 0.07 - Training acc: 98.02 - t: 4.44 sec\n",
      "Epoch: [21/50] - Loss: 0.13 - Training acc: 98.24 - t: 4.43 sec\n",
      "Epoch: [22/50] - Loss: 0.06 - Training acc: 98.62 - t: 4.46 sec\n",
      "Epoch: [23/50] - Loss: 0.11 - Training acc: 99.00 - t: 4.44 sec\n",
      "Epoch: [24/50] - Loss: 0.06 - Training acc: 99.28 - t: 4.44 sec\n",
      "Epoch: [25/50] - Loss: 0.08 - Training acc: 99.37 - t: 4.44 sec\n",
      "Epoch: [26/50] - Loss: 0.05 - Training acc: 99.63 - t: 4.44 sec\n",
      "Epoch: [27/50] - Loss: 0.04 - Training acc: 99.74 - t: 4.44 sec\n",
      "Epoch: [28/50] - Loss: 0.04 - Training acc: 99.63 - t: 4.46 sec\n",
      "Epoch: [29/50] - Loss: 0.03 - Training acc: 99.75 - t: 4.46 sec\n",
      "Epoch: [30/50] - Loss: 0.05 - Training acc: 99.45 - t: 4.44 sec\n",
      "Epoch: [31/50] - Loss: 0.04 - Training acc: 99.60 - t: 4.45 sec\n",
      "Epoch: [32/50] - Loss: 0.02 - Training acc: 99.82 - t: 4.44 sec\n",
      "Epoch: [33/50] - Loss: 0.02 - Training acc: 99.85 - t: 4.44 sec\n",
      "Epoch: [34/50] - Loss: 0.02 - Training acc: 99.88 - t: 4.43 sec\n",
      "Epoch: [35/50] - Loss: 0.02 - Training acc: 99.89 - t: 4.43 sec\n",
      "Epoch: [36/50] - Loss: 0.01 - Training acc: 99.89 - t: 4.41 sec\n",
      "Epoch: [37/50] - Loss: 0.01 - Training acc: 99.89 - t: 4.45 sec\n",
      "Epoch: [38/50] - Loss: 0.01 - Training acc: 99.89 - t: 4.42 sec\n",
      "Epoch: [39/50] - Loss: 0.01 - Training acc: 99.89 - t: 4.47 sec\n",
      "Epoch: [40/50] - Loss: 0.01 - Training acc: 99.89 - t: 4.46 sec\n",
      "Epoch: [41/50] - Loss: 0.01 - Training acc: 99.89 - t: 4.40 sec\n",
      "Epoch: [42/50] - Loss: 0.01 - Training acc: 99.89 - t: 4.44 sec\n",
      "Epoch: [43/50] - Loss: 0.01 - Training acc: 99.89 - t: 4.46 sec\n",
      "Epoch: [44/50] - Loss: 0.01 - Training acc: 99.91 - t: 4.45 sec\n",
      "Epoch: [45/50] - Loss: 0.01 - Training acc: 99.89 - t: 4.42 sec\n",
      "Epoch: [46/50] - Loss: 0.01 - Training acc: 99.89 - t: 4.44 sec\n",
      "Epoch: [47/50] - Loss: 0.00 - Training acc: 99.91 - t: 4.45 sec\n",
      "Epoch: [48/50] - Loss: 0.00 - Training acc: 99.91 - t: 4.46 sec\n",
      "Epoch: [49/50] - Loss: 0.00 - Training acc: 99.91 - t: 4.45 sec\n",
      "SHD Test Accuracy: 68.14\n"
     ]
    }
   ],
   "source": [
    "args.train_alpha = True\n",
    "args.hierarchy_tau = True\n",
    "train_loss, test_acc_shd, test_acc_ssc, weight = train_mosaic(key = jax.random.PRNGKey(args.seed), n_batch=args.batch_size, n_epochs=50, args = args, \n",
    "                                                                lr = args.lr, lr_dropstep=1., \n",
    "                                                                train_dl = train_loader_custom_collate, test_dl = test_loader_custom_collate, \n",
    "                                                                model=hsnn_step, param_initializer=params_initializer, decoder=decoder_vmax, \n",
    "                                                                noise_start_step=10, noise_std=0.1,\n",
    "                                                                target_fr=None, lambda_fr=None, dataset_name='shd')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "denram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
