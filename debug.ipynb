{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.example_libraries import optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "from jax import vmap, pmap, jit, value_and_grad, local_device_count\n",
    "from jax.example_libraries import optimizers\n",
    "from jax.lax import scan, cond\n",
    "import pickle\n",
    "\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\".75\" # needed because network is huge\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters of the SNN model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_initialization import args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Import the SHD dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_dataset import get_dataloader\n",
    "train_loader_custom_collate, val_loader_custom_collate, test_loader_custom_collate = get_dataloader( args=args, verbose=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_spikes = []\n",
    "for step, (x, y) in enumerate(test_loader_custom_collate):\n",
    "    mean_spikes.append( x.mean(axis=(0,1)) )\n",
    "mean_activity = np.mean(mean_spikes, axis=0)\n",
    "# np.mean( mean_activity ), np.max(mean_activity), np.min(mean_activity), mean_activity.shape\n",
    "\n",
    "print('Our Dataloader')\n",
    "print(f'Mean Firing Rate on Test set: {np.mean( mean_activity )}')\n",
    "act_channels = []\n",
    "for i in range(7):\n",
    "    print( f'Channels {i*100}-{(i+1)*100} Firing rate: { np.mean( mean_activity[i*100:(i+1)*100] ) }' )\n",
    "    act_channels.append( np.mean( mean_activity[i*100:(i+1)*100] ) )\n",
    "\n",
    "\n",
    "# Loading dataloader from Bittar\n",
    "from spikin_datasets import load_shd_or_ssc\n",
    "test_loader_sparch = load_shd_or_ssc( \n",
    "    dataset_name = 'shd',\n",
    "    data_folder = '/Users/filippomoro/Desktop/KINGSTONE/Datasets/SHD/audiospikes',\n",
    "    split = 'test',\n",
    "    batch_size = 128,\n",
    "    nb_steps=100,\n",
    "    shuffle=False,\n",
    "    workers=0,\n",
    " )\n",
    "\n",
    "mean_spikes_bittar = []\n",
    "for step, (x, y) in enumerate(test_loader_sparch):\n",
    "    mean_spikes_bittar.append( x.mean(axis=(0,1)) )\n",
    "mean_activity_bittar = np.mean(mean_spikes_bittar, axis=0)\n",
    "# np.mean( mean_activity ), np.max(mean_activity), np.min(mean_activity), mean_activity.shape\n",
    "print('\\nBittar Dataloader')\n",
    "print(f'Mean Firing Rate on Test set: {np.mean( mean_activity_bittar )}')\n",
    "act_channels_bittar = []\n",
    "for i in range(7):\n",
    "    print( f'Channels {i*100}-{(i+1)*100} Firing rate: { np.mean( mean_activity_bittar[i*100:(i+1)*100] ) }' )\n",
    "    act_channels_bittar.append( np.mean( mean_activity_bittar[i*100:(i+1)*100] ) )\n",
    "\n",
    "fig, ax = plt.subplots( )\n",
    "ax.bar( np.arange(7), act_channels, alpha=0.75, width=0.25, label='OUR' )\n",
    "ax.bar( np.arange(7)+0.25, act_channels_bittar, alpha=0.75, width=0.25, label='Bittar' )\n",
    "ax.set_ylabel('Firing Rate [a.u.]', size=12)\n",
    "ax.set_xlabel('Channel-band [x100]', size=12)\n",
    "ax.legend(prop={'size':12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from spikin_datasets import SpikingDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from utils_dataset import custom_collate_fn\n",
    "\n",
    "# import dataset\n",
    "train_ds = SpikingDataset('shd', '/Users/filippomoro/Desktop/KINGSTONE/Datasets/SHD/audiospikes', 'train', args.nb_steps) # print(len(train_ds[0]))\n",
    "test_ds  = SpikingDataset('shd', '/Users/filippomoro/Desktop/KINGSTONE/Datasets/SHD/audiospikes', 'test', args.nb_steps) # print(len(train_ds[0]))\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "\n",
    "train_size = int(0.99 * len(train_ds))\n",
    "val_size   = len(train_ds) - train_size\n",
    "# train_ds_split = train_ds\n",
    "train_ds_split, val_ds_split = random_split(train_ds, [train_size, val_size])\n",
    "print(f'Train DL size: {len(train_ds)}, Validation DL size: {len(val_ds_split)}, Test DL size: {len(test_ds)}')\n",
    "\n",
    "train_loader_custom_collate = DataLoader(train_ds_split, args.batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader_custom_collate   = DataLoader(val_ds_split,   args.batch_size, shuffle=None, collate_fn=custom_collate_fn)\n",
    "test_loader_custom_collate  = DataLoader(test_ds,        args.batch_size, shuffle=None, collate_fn=custom_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from utils_initialization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lif_step( args_in, input_spikes ):\n",
    "    ''' Forward function for the Leaky-Integrate and Fire neuron layer, adopted here for the hidden layers. '''\n",
    "    net_params, net_states = args_in\n",
    "    # state: the parameters (weights) and the state of the neurons (spikes, inputs and membrane, ecc..)\n",
    "    w, alpha = net_params; w_mask, tau, V_mem, out_spikes, v_thr, noise_sd = net_states\n",
    "\n",
    "    # V_mem = (alpha) * (V_mem - out_spikes) + (1-alpha) * I_in #- out_spikes*v_thr\n",
    "    # V_mem = (alpha) * (V_mem - out_spikes) + I_in #- out_spikes*v_thr\n",
    "    V_mem = alpha * V_mem + input_spikes - out_spikes*v_thr\n",
    "    out_spikes = spiking_fn( V_mem, v_thr )\n",
    "    \n",
    "    return [ [w, alpha], [w_mask, tau, V_mem, out_spikes, v_thr, noise_sd] ], out_spikes\n",
    "\n",
    "\n",
    "# Leaky Integrate and Fire layer, Recurrent\n",
    "def rlif_step( args_in, input_spikes):\n",
    "    ''' Forward function for the Leaky-Integrate and Fire neuron layer, adopted here for the hidden layers. '''\n",
    "    net_params, net_states = args_in\n",
    "    # state: the parameters (weights) and the state of the neurons (spikes, inputs and membrane, ecc..)\n",
    "    w, alpha = net_params; w_mask, tau, V_mem, out_spikes, v_thr, noise_sd = net_states\n",
    "    win_mask, wrec_mask = w_mask\n",
    "    if len(w) == 3: # it means that we'll do normalization\n",
    "        weight, scale, bias = w\n",
    "    else: weight = w\n",
    "    win, wrec = weight\n",
    "    w_rec_diag_zeros = jnp.ones_like(wrec) - jnp.eye( wrec.shape[0] )\n",
    "\n",
    "    # we evolve the state of the neuron according to the LIF formula, Euler approximation\n",
    "    I_rec = jnp.matmul(out_spikes, wrec*wrec_mask*w_rec_diag_zeros)\n",
    "    # V_mem = (alpha) * (V_mem) + (1-alpha) * I_in - out_spikes*v_thr\n",
    "    V_mem = alpha * V_mem + input_spikes + I_rec - out_spikes*v_thr\n",
    "    # V_mem = alpha * (V_mem - out_spikes) + (1-alpha) * ( I_in_norm )\n",
    "    # V_mem = alpha * (V_mem - out_spikes) + (1) * ( I_in_norm )\n",
    "    out_spikes = spiking_fn( V_mem, v_thr )\n",
    "    \n",
    "    return [ [w, alpha], [w_mask, tau, V_mem, out_spikes, v_thr, noise_sd] ], out_spikes\n",
    "\n",
    "# Leaky Integrator (output layer)\n",
    "def li_step(args_in, input_spikes):\n",
    "    ''' Forward function for the Leaky-Integrator neuron layer, adopted here for the output layers. '''\n",
    "    net_params, net_states = args_in\n",
    "    # state: the parameters (weights) and the state of the neurons (inputs and membrane)\n",
    "    w, alpha = net_params; w_mask, tau, V_mem, out_spikes, v_thr, noise_sd = net_states\n",
    "\n",
    "    # we evolve the state of the neuron according to the LI formula, Euler approximation\n",
    "    V_mem = (alpha) * (V_mem) + (1-alpha) * input_spikes\n",
    "    # V_mem = (alpha) * (V_mem) + input_spikes\n",
    "    \n",
    "    return [ [w, alpha], [w_mask, tau, V_mem, out_spikes, v_thr, noise_sd] ], V_mem\n",
    "\n",
    "# parallelizing the Single Layer\n",
    "@jit\n",
    "def scan_layer( args_in, input_spikes ):\n",
    "    args_out_layer, out_spikes_layer = scan( layer, args_in, input_spikes, length=args.nb_steps )\n",
    "    return args_out_layer, out_spikes_layer\n",
    "vscan_layer = vmap( scan_layer, in_axes=(None, 0))\n",
    "\n",
    "@jit\n",
    "def scan_out_layer( args_in, input_spikes ):\n",
    "    args_out_layer, out_spikes_layer = scan( layer_out, args_in, input_spikes, length=args.nb_steps )\n",
    "    return args_out_layer, out_spikes_layer\n",
    "vscan_layer_out = vmap( scan_out_layer, in_axes=(None, 0))\n",
    "\n",
    "@jit\n",
    "def hsnn( args_in, input_spikes ):\n",
    "    net_params, net_states, key, dropout_rate = args_in\n",
    "    n_layers = len( net_params )\n",
    "    # collection of output spikes\n",
    "    out_spike_net = []\n",
    "    # Loop over the layers\n",
    "    for l in range(n_layers):\n",
    "        if l == 0: layer_input_spike = input_spikes\n",
    "        else: layer_input_spike = out_spikes_layer\n",
    "        # making layers' params and states explitic\n",
    "        # parameters (weights and alpha) and the state of the neurons (spikes, inputs and membrane, ecc..)\n",
    "        w, alpha = net_params[l]; w_mask, tau, V_mem, out_spikes, v_thr, noise_sd = net_states[l]\n",
    "        if len(w) == 3: # it means that we'll do normalization\n",
    "            weight, scale, bias = w\n",
    "        else: weight = w\n",
    "        if len(weight) ==2: weight, _ = weight\n",
    "        # we evolve the state of the neuron according to the LIF formula, Euler approximation\n",
    "        I_in = jnp.matmul(layer_input_spike, weight)\n",
    "        # Normalization (if selected)\n",
    "        if len(w) == 3: # it means that we'll do normalization\n",
    "            b, t, n = I_in.shape\n",
    "            I_in = norm( I_in.reshape( b*t, n ), bias = bias, scale = scale )\n",
    "            I_in = I_in.reshape( b,t,n ) # normalized input current\n",
    "        # Forward pass of the Layer\n",
    "        args_in = [net_params[l], net_states[l]]\n",
    "        if l+1 == n_layers:\n",
    "            _, out_spikes_layer = vscan_layer_out( args_in, I_in )\n",
    "        else: \n",
    "            _, out_spikes_layer = vscan_layer( args_in, I_in )\n",
    "            # Dropout\n",
    "            key, key_dropout = jax.random.split(key, 2)\n",
    "            out_spikes_layer = dropout( key_dropout, out_spikes_layer, rate=dropout_rate, deterministic=False )\n",
    "        out_spike_net.append(out_spikes_layer)\n",
    "    return out_spikes_layer, out_spike_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( out_spikes_layer[0].T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( I_in[0].T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow( out_spikes_vlayer[10].T )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
